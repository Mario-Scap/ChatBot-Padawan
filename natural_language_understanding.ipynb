{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import import_ipynb\n",
    "from spacy import displacy\n",
    "import random\n",
    "import categories\n",
    "from categories import *\n",
    "from categories import features_list\n",
    "from categories import question_list\n",
    "import polyfuzz as pf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devo trovare tutte le dipendenze per una certa caratteristica dei padawain\n",
    "class NLU: \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.doc = self.nlp(self.text)\n",
    "        #self.category = self.categories.categories\n",
    "        #self.feature_list = self.category.get_features()\n",
    "        #self.feature_list = set(self.categories.get_features())\n",
    "        \n",
    "        \n",
    "    #cerca le dipendenze di tutte le parole (pos tagging)\n",
    "    def find_dependency(self):\n",
    "        dict_of_dependency = {}\n",
    "        for token in self.doc:\n",
    "            dict_of_dependency[token.text] = token.dep_, token.pos_, token.head.text, token.morph\n",
    "        \n",
    "        return dict_of_dependency\n",
    "\n",
    "    #visualizza le dipendenze\n",
    "    def visualize_dependency(self):\n",
    "        displacy.render(self.doc, style=\"dep\")\n",
    "        return self.doc\n",
    "\n",
    "    #vado a fare le regex per le domande. La regex deve andare a individuare 3 cose: eventuale punto di domande, la categoria e la feature, e la negazione eventualmente\n",
    "    def regular_expression_questions(self):\n",
    "        questions = [question.split() for question in question_list] #prendo la lista delle domande per ogni categoria\n",
    "        features = [feature for feature in features_list] #prendo la lista delle features della specifica categoria\n",
    "        found = [] #lista delle domande trovate\n",
    "        pattern_1 = \"[?]\"  #primo pattern in cui vado a vedere se rappresenta una domanda\n",
    "        #pattern_2 = f\"{self.category}\" #lista delle categorie\n",
    "        pattern_3 = f\"{features}\"#lista delle caratteristiche\n",
    "\n",
    "        for q in questions:\n",
    "            x = re.search(pattern_1, q[0]) #vado a vedere se ce' il punto di domanda\n",
    "            if x:\n",
    "                #re.search(pattern_2, q[0]) #vado a vedere se c'e' la categoria\n",
    "                re.search(pattern_3, q[0]) #vado a vedere se c'e' la feature\n",
    "                found.append(q[0])\n",
    "\n",
    "            else:\n",
    "                found.append(None) #se non c'e' nessuna domanda allora metto None\n",
    "\n",
    "\n",
    "    #Regex per le risposte. La regex deve andare a individuare la features presente nella risposta\n",
    "    def regular_expression_answers(self):    \n",
    "        features = [feature.split() for feature in features_list] #prendo la lista delle features della specifica categoria\n",
    "        self.pos = True\n",
    "        correct = [] #lista delle risposte corrette\n",
    "        #pattern_1 = f\"{self.category}\" #lista delle categorie\n",
    "        pattern_2 = f\"{features}\"#lista delle caratteristiche\n",
    "\n",
    "        for token in self.doc:\n",
    "            x = re.search(pattern_2, token.text) #vado a vedere la feature nel testo\n",
    "            if x:\n",
    "                correct.append(token.text)\n",
    "            else:\n",
    "                correct.append(None)\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "    #vado a controllare le dependency per le features dei padawain\n",
    "    def check_dependency_padawian(self):\n",
    "        check_list = [feature.split() for feature in features_list] #prendo la lista delle feature della specifica categoria\n",
    "        correct = []\n",
    "\n",
    "        for token in self.doc: #vado nel testo\n",
    "            for f in check_list: #vado nella lista delle features\n",
    "                if token.text == f[0]: #se la feature è uguale al token    \n",
    "                    if len(f) == 1:\n",
    "                        correct.append(token.text)\n",
    "                    \n",
    "                    #se e' una parola composta allora non va bene\n",
    "                    elif token.i == len(self.doc) - 1 and len(f) > 1 :\n",
    "                        correct.append(None)\n",
    "                    \n",
    "                    else:\n",
    "                        for i in range(1, len(f)):\n",
    "                            if token.nbor(i).text != f[i]:\n",
    "                                correct.append(None)\n",
    "                            \n",
    "                            if token.i == len(self.doc) - 1 and len(f) > 1 :\n",
    "                                correct.append(None)\n",
    "                        \n",
    "                        return \"\".join(f)\n",
    "                            \n",
    "        correct = [x for x in correct if x is not None]\n",
    "        return correct\n",
    "    \n",
    "     #fare funzione che prende la risposta che ci si aspetta e la confronta con la risposta data dall'utente\n",
    "    def check_response(self, response, correct_response):\n",
    "        max = 0\n",
    "        for answer in correct_response:\n",
    "            #controllo della similiarità con polyfuzz\n",
    "            model = pf.Model(\"TF-IDF\")\n",
    "            model.match([answer, response])\n",
    "            print(model.get_matches())\n",
    "            sim = model.get_matches().at[0,\"Similarity\"]\n",
    "            if sim > max:\n",
    "                max = sim\n",
    "\n",
    "        return sim\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def features_in_text(self):\n",
    "       return len(self.check_dependency_padawian())\n",
    "    \"\"\"\n",
    "\n",
    "#Praticamente cerco di capire se la frase che e' stata detta e' positiva o negativa. In questo modo nel dialog system gestisco la tipologia di risposta\n",
    "\"\"\"\n",
    "    def check_response_type(self):\n",
    "        self.pos = True\n",
    "        for token in self.doc:\n",
    "            if token.text == 'no' or token.text == 'not' or \"Neg\" in token.morph.get(\"Polarity\"):\n",
    "                self.pos = False\n",
    "            \n",
    "        return self.pos\n",
    "\"\"\"       \n",
    "if __name__ == \"__main__\":\n",
    "    sent = NLU(\"I am Humanoid and i have force and want to learn to use the lightsaber\") #Piccola frase di esempio. In questo caso va a pescare dalla losta force e lightsaber\n",
    "    print(sent.text)\n",
    "    print(\"\\n\")\n",
    "    #print(\"Visualize dependency: \", sent.visualize_dependency())\n",
    "    dependencies = sent.find_dependency()\n",
    "\n",
    "    for key, value in dependencies.items():\n",
    "        print(f\"{key}, {value}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f\"Parole chiave: {sent.check_dependency_padawian()}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Numero Parole Chiave: {sent.features_in_text()}\")\n",
    "    #print(f\"{sent.check_response_type()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
